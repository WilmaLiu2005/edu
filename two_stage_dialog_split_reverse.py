# å…ˆæ—¶é—´é—´éš”15åˆ†é’Ÿï¼Œç„¶åå†LLMåˆ’åˆ†
from openai import OpenAI
import random
import time
import os
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import json5
import pandas as pd
import csv
# ==============================    

MAX_WORKERS = 50
MAX_RETRIES = 3
INITIAL_RETRY_DELAY = 1
MAX_RETRY_DELAY = 3
API_KEY = "sk-0jErqj61bIYM135CEqhfj318rKIM1TIa"  # å¡«å†™ä½ çš„ API key
BASE_URL = "https://api-gateway.glm.ai/v1"
MODEL_NAME = "gpt-5-2025-08-07"  # æˆ–ä½ è‡ªå·±çš„æ¨¡å‹
INPUT_FOLDER = "/Users/vince/undergraduate/KEG/edu/Data/pre_validation5"   # è¾“å…¥ CSV æ–‡ä»¶å¤¹
OUTPUT_FOLDER = "/Users/vince/undergraduate/KEG/edu/test_split_reverse" # è¾“å‡º CSV æ–‡ä»¶å¤¹

os.makedirs(OUTPUT_FOLDER, exist_ok=True)

def robust_read_csv(file_path, text_columns=None):
    """
    å®‰å…¨è¯»å– CSV æ–‡ä»¶ï¼Œé€‚ç”¨äºå«æœ‰æ¢è¡Œç¬¦ã€Markdown å›¾ç‰‡é“¾æ¥ã€åŒå¼•å·ç­‰æƒ…å†µã€‚
    å‚æ•°ï¼š
        file_path: CSV æ–‡ä»¶è·¯å¾„
        text_columns: éœ€è¦å¤„ç†æ¢è¡Œç¬¦çš„æ–‡æœ¬åˆ—åˆ—è¡¨ï¼Œä¾‹å¦‚ ["æé—®å†…å®¹", "AIå›å¤"]
    è¿”å›ï¼š
        pd.DataFrame æˆ– Noneï¼ˆç©ºæ–‡ä»¶æˆ–æ ¼å¼å¼‚å¸¸ï¼‰
    """
    try:
        df = pd.read_csv(
            file_path,
            encoding="utf-8-sig",
            engine="python",       # æ›´çµæ´»ï¼Œæ”¯æŒå¤šç§å¤æ‚ CSV
            quotechar='"',         # å¼•å·åŒ…è£¹çš„å­—æ®µ
            doublequote=True,      # åŒå¼•å·è½¬ä¹‰
            keep_default_na=False, # ä¿ç•™ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯ NaN
        )
    except Exception as e:
        print(f"âš ï¸ è¯»å– CSV å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return None

    if df.empty:
        print(f"âš ï¸ æ–‡ä»¶ {file_path} æ˜¯ç©ºçš„")
        return None

    # å»é™¤åˆ—åå‰åç©ºæ ¼
    df.columns = df.columns.str.strip()

    # æ£€æŸ¥å¿…è¦åˆ—
    required_cols = ["è¡Œåºå·", "æé—®æ—¶é—´"]
    for col in required_cols:
        if col not in df.columns:
            print(f"âš ï¸ æ–‡ä»¶ {file_path} ç¼ºå°‘å¿…è¦åˆ—: {col}")
            return None

    # å¤„ç†æ–‡æœ¬åˆ—æ¢è¡Œç¬¦
    if text_columns:
        for col in text_columns:
            if col in df.columns:
                df[col] = df[col].astype(str).str.replace("\n", " ", regex=False)

    # è½¬æ¢è¡Œåºå·ä¸ºæ•´æ•°
    df["è¡Œåºå·"] = pd.to_numeric(df["è¡Œåºå·"], errors="coerce")
    df = df.dropna(subset=["è¡Œåºå·"])
    df["è¡Œåºå·"] = df["è¡Œåºå·"].astype(int)

    # è½¬æ¢æ—¶é—´åˆ—
    df["æé—®æ—¶é—´"] = pd.to_datetime(df["æé—®æ—¶é—´"], errors="coerce")
    df = df.dropna(subset=["æé—®æ—¶é—´"])

    if df.empty:
        print(f"âš ï¸ æ–‡ä»¶ {file_path} å…¨éƒ¨è¡Œæ— æ•ˆï¼ˆè¡Œåºå·æˆ–æé—®æ—¶é—´è§£æå¤±è´¥ï¼‰")
        return None

    df = df.sort_values("æé—®æ—¶é—´").reset_index(drop=True)
    return df

# ==============================
# GPT API è°ƒç”¨
# ==============================
def gpt_api_call(messages, model=MODEL_NAME):
    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    for attempt in range(MAX_RETRIES):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                max_completion_tokens=10000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {str(e)}")
            if attempt < MAX_RETRIES - 1:
                delay = min(INITIAL_RETRY_DELAY * (2 ** attempt) + random.uniform(0, 1), MAX_RETRY_DELAY)
                time.sleep(delay)
            else:
                return None

# ==============================
# JSON è§£æï¼ˆé²æ£’ï¼‰
# ==============================
def robust_json_parse(text):
    cleaned = re.sub(r"^```(?:json)?|```$", "", text.strip(), flags=re.MULTILINE).strip()
    try:
        return json.loads(cleaned)
    except Exception:
        try:
            return json5.loads(cleaned)
        except Exception:
            print("âš ï¸ JSON è§£æå¤±è´¥ï¼ŒåŸå§‹è¾“å‡ºï¼š", text[:200])
            return {}

# ==============================
# Stage 1: æ—¶é—´åˆ’åˆ†
# ==============================
def time_based_split(df, time_threshold=15):
    results = []
    if df.empty:
        return results

    current_chunk = [df.iloc[0]]
    for i in range(1, len(df)):
        delta = (df.iloc[i]["æé—®æ—¶é—´"] - df.iloc[i - 1]["æé—®æ—¶é—´"]).total_seconds() / 60
        if delta > time_threshold:
            results.append(pd.DataFrame(current_chunk))
            current_chunk = [df.iloc[i]]
        else:
            current_chunk.append(df.iloc[i])
    if current_chunk:
        results.append(pd.DataFrame(current_chunk))
    return results

# ==============================
# Stage 2: LLM åˆ’åˆ†
# ==============================
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªå¯¹è¯åˆ†æåŠ©æ‰‹ã€‚
ä»¥ä¸‹æ˜¯æŸä¸ªå­¦ç”Ÿå’Œ AI å­¦ä¼´çš„å®Œæ•´äº¤äº’è®°å½•ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—ã€‚
æ¯æ¡è®°å½•åŒ…å«:
- è¡Œåºå·
- æé—®æ—¶é—´
- æé—®å†…å®¹
- AIå›å¤

è¯·å°†è¿™äº›äº¤äº’åˆ’åˆ†æˆå¤šæ®µç‹¬ç«‹çš„å¯¹è¯ï¼ˆsessionï¼‰ã€‚  
åˆ’åˆ†è§„åˆ™ï¼š
1. å¿…é¡»ä¿æŒåŸå§‹é¡ºåºï¼Œä¸èƒ½æ‰“ä¹±æˆ–è·³è·ƒ
2. å¦‚æœä¸¤æ¡äº¤äº’ä¹‹é—´çš„æ—¶é—´é—´éš”è¿‡é•¿ï¼Œå°±ä¸€å®šè¦åˆ†æˆæ–°çš„å¯¹è¯
3. å¦‚æœè¯é¢˜å‘ç”Ÿæ˜æ˜¾å˜åŒ–ï¼Œä¹Ÿè¦å¼€å¯æ–°å¯¹è¯
4. å¦åˆ™è§†ä¸ºåŒä¸€æ®µå¯¹è¯  

è¾“å‡ºæ ¼å¼ï¼š
- JSON æ ¼å¼ï¼š{{è¡Œå·: å¯¹è¯ç¼–å·}}ï¼Œå¯¹è¯ç¼–å·ä» 1 å¼€å§‹  
- ä¸è¦è¾“å‡ºå…¶ä»–è¯´æ˜æˆ–æ–‡å­—  

äº¤äº’è®°å½•å¦‚ä¸‹ï¼š
{dialogues_json}
"""

def llm_split(group_df):
    dialogues = []
    for idx, row in group_df.iterrows():
        dialogues.append({
            "è¡Œå·": int(idx) + 1,
            "æé—®æ—¶é—´": str(row["æé—®æ—¶é—´"]),
            "æé—®å†…å®¹": str(row["æé—®å†…å®¹"]),
            "AIå›å¤": str(row["AIå›å¤"])
        })
    dialogues_json = json.dumps(dialogues, ensure_ascii=False)
    prompt = PROMPT_TEMPLATE.format(dialogues_json=dialogues_json)

    messages = [
        {"role": "system", "content": "ä½ æ˜¯äº¤äº’è½®æ¬¡åˆ†å‰²åŠ©æ‰‹"},
        {"role": "user", "content": prompt}
    ]
    response = gpt_api_call(messages)
    if not response:
        return {}
    return robust_json_parse(response)

# ==============================
# å¤„ç†å•ä¸ª CSV
# ==============================
def process_csv(file_path, output_folder):
    try:
        # è¯»å– CSV
        df = robust_read_csv(file_path, text_columns=["æé—®å†…å®¹", "AIå›å¤"])
        if df is None or df.empty:
            print(f"âš ï¸ {file_path} æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
            return

        student_id = df["å­¦ç”ŸID"].iloc[0] if "å­¦ç”ŸID" in df.columns else os.path.splitext(os.path.basename(file_path))[0]

        # Stage 1: æ—¶é—´é—´éš”åˆ’åˆ†
        time_splits = time_based_split(df)  # æ¯å—é¡ºåºä¿æŒä¸å˜

        file_index = 1  # å…¨å±€ç¼–å·

        # Stage 2: LLM åˆ’åˆ†
        for time_idx, group in enumerate(time_splits, start=1):
            mapping = llm_split(group)
            # print(mapping)
            if not mapping:
                print(f"âš ï¸ {student_id} æ—¶é—´ç‰‡ {time_idx} LLM åˆ’åˆ†å¤±è´¥ï¼Œè·³è¿‡")
                continue

            # å°† LLM è¾“å‡ºçš„ session_id é¡ºåºæ˜ å°„åˆ°é¢„å¯¹è¯é¡ºåº
            session_ids = []
            prev_session = 1
            for idx in range(len(group)):
                key = str(idx + 1)  # LLM çš„è¡Œå·é€šå¸¸ä» 1 å¼€å§‹
                sid = mapping.get(key, prev_session)
                session_ids.append(sid)
                prev_session = sid

            # æ ¹æ® session_id åˆ‡åˆ†å­ä¼šè¯
            current_session = []
            current_id = session_ids[0]
            for idx, sid in enumerate(session_ids):
                if sid != current_id:
                    # ä¿å­˜ä¸Šä¸€æ®µå­ä¼šè¯
                    sub_group = group.iloc[current_session].copy().reset_index(drop=True)
                    output_csv = os.path.join(output_folder, f"{student_id}_{file_index}.csv")
                    sub_group.to_csv(output_csv, index=False, encoding="utf-8", quoting=csv.QUOTE_ALL)
                    print(f"âœ… å·²ç”Ÿæˆ {output_csv}, å…± {len(sub_group)} è¡Œ")
                    file_index += 1

                    # å¼€å¯æ–°å­ä¼šè¯
                    current_session = [idx]
                    current_id = sid
                else:
                    current_session.append(idx)

            # ä¿å­˜æœ€åä¸€æ®µå­ä¼šè¯
            if current_session:
                sub_group = group.iloc[current_session].copy().reset_index(drop=True)
                output_csv = os.path.join(output_folder, f"{student_id}_{file_index}.csv")
                sub_group.to_csv(output_csv, index=False, encoding="utf-8", quoting=csv.QUOTE_ALL)
                print(f"âœ… å·²ç”Ÿæˆ {output_csv}, å…± {len(sub_group)} è¡Œ")
                file_index += 1

    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} å‡ºé”™: {e}")


# ==============================
# ä¸»ç¨‹åº
# ==============================
def main():
    files = [f for f in os.listdir(INPUT_FOLDER) if f.endswith(".csv")]
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_csv, os.path.join(INPUT_FOLDER, f), OUTPUT_FOLDER): f for f in files}
        for future in as_completed(futures):
            file = futures[future]
            try:
                future.result()
            except Exception as e:
                print(f"âŒ å¤„ç† {file} å‡ºé”™: {e}")

    print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()